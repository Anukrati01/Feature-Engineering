{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "5dVeMrF7xb5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "  -  a parameter is an internal variable within a model that is learned from the training data during the model's training process, influencing its performance and behavior.\n",
        "   \n",
        "  **Example:-**\n",
        "  In linear regression, parameters are the coefficients (weights) of the features, while in neural networks, they include weights and biases."
      ],
      "metadata": {
        "id": "tVJi8mj2WkdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "   - **Correlation:-**  Correlation is a statistical term that describes the relationship or association between two or more variables.\n",
        "It indicates whether the variables tend to move together (positive correlation), move in opposite directions (negative correlation), or have no relationship (zero correlation).\n",
        "     \n",
        "     **negative correlation:-**Negative correlation, also known as inverse correlation, means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "For example, if the price of a product increases, the demand for that product might decrease, indicating a negative correlation between price and demand.\n"
      ],
      "metadata": {
        "id": "PRij1_iQWmV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning.\n",
        "   - **Machine Learning:-** Machine learning is a field of artificial intelligence (AI) that enables computers to learn from data and improve their performance on specific tasks without explicit programming. The main components of machine learning are data, algorithms, models, and predictions.\n",
        "      \n",
        "       **components of machine learning:-**\n",
        "        \n",
        "        **Representation**\n",
        "This refers to the way knowledge is represented for ML purposes. Some examples include decision trees, sets of rules, instances, graphical models, neural networks, support vector machines, model ensembles, and various others.\n",
        "\n",
        "    **Abstraction**\n",
        "Abstraction simplifies the representation of a problem, allowing for more efficient problem-solving with reduced memory and computation requirements. Examples of data abstraction are decreasing the spatial and temporal resolution or dividing continuous variables into meaningful ranges that align with specific goals.\n",
        "\n",
        "    **Evaluation**\n",
        "Every ML project needs a method for evaluating hypotheses. Some examples are accuracy, prediction and recall, squared error, KL divergence (relative entropy), and others.\n",
        "\n",
        "    **Generalization**\n",
        "Generalization is crucial for a model to effectively handle new, unfamiliar data that comes from the same distribution as the data used to train the model. It allows teams to gain a deeper understanding of overfitting and assess the quality of a model.\n",
        "\n",
        "    **Data Storage**\n",
        "This one might easily get forgotten among the components of machine learning, but where your data resides is very important. Common storage solutions for machine learning include object storage, distributed file systems, and cloud-based storage."
      ],
      "metadata": {
        "id": "DuEKYaP1Wmue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "    - A model's loss value, which quantifies the difference between its predictions and actual values, helps determine how well the model is performing and guides its improvement during training. A lower loss value generally indicates a better-performing model.\n"
      ],
      "metadata": {
        "id": "1WRLISXgWm9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "   - **Continuous Variables:**  \n",
        " These variables represent quantities that can be measured and can take on any value within a certain range, including decimals and fractions.\n",
        "     \n",
        "     **Examples:** Height, weight, temperature, time, distance, salary, age.\n",
        "    **Characteristics:**\n",
        "      1. Can be measured on a continuous scale.\n",
        "      2. Have an infinite number of possible values within a given range.\n",
        "      3. Often represented numerically.\n",
        "      \n",
        "      **Categorical Variables:**\n",
        "\n",
        "     These variables represent qualities or characteristics that fall into distinct categories or groups.\n",
        "    \n",
        "    **Examples:**\n",
        "Gender (male/female), blood type (A, B, AB, O), color (red, blue, green), type of car (sedan, SUV, truck), yes/no questions.\n",
        "    \n",
        "    **Characteristics:**\n",
        "     1. Represent qualitative data.\n",
        "     2.Can be further categorized as nominal, ordinal, or dichotomous.\n",
        "     3.Do not have a numerical value, but rather represent categories or labels.\n"
      ],
      "metadata": {
        "id": "uTVSUDaJWnZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "    - To handle categorical variables in machine learning, you can use techniques like one-hot encoding for nominal data (no inherent order) and ordinal encoding for ordinal data (categories with a meaningful order), or even label encoding for simple conversion.\n",
        "       **Common Encoding Techniques**\n",
        "\n",
        "     **One-Hot Encoding:**\n",
        "      1. Creates a new binary column for each category in the dataset.\n",
        "      2. Assigns '1' if the observation belongs to that category and '0' otherwise.\n",
        "      3. Ideal for nominal data.\n",
        "      4. Can lead to the \"curse of dimensionality\" with a large number of categories.\n",
        "     **Ordinal Encoding:**\n",
        "      1. Assigns a unique integer to each category based on its order.\n",
        "      2. Suitable for ordinal data where the order matters.\n",
        "     **Label Encoding:**\n",
        "       1. Assigns a unique integer to each category, but without considering any order.\n",
        "      2. Simple to implement, but might not be suitable for all scenarios.\n",
        " **Binary Encoding:**\n",
        "      1. Converts categories into binary digits, reducing dimensionality compared to one-hot encoding.\n",
        "     2. Useful for high-cardinality datasets.\n",
        "     **Frequency Encoding:**\n",
        "     \n",
        "          1.Replaces each category with the frequency (or count) of its occurrences in the dataset.\n",
        "          \n",
        "          2.Can be useful for identifying important categories.\n",
        "     \n",
        "      **Target Encoding:**\n",
        "\n",
        "        1. Replaces each category with the mean of the target variable for that category.\n",
        "        2. Can improve model performance, but requires careful handling to avoid overfitting."
      ],
      "metadata": {
        "id": "GuQmzO-3WnsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "   - training\" a dataset means using a portion of the data to teach a model to recognize patterns and relationships, while \"testing\" a dataset means evaluating how well the trained model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "AtAr03Cpf5ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "   - \"preprocessing\" refers to the process of transforming raw data into a format that is more suitable for machine learning algorithms, using utility functions and transformer classes within the sklearn.preprocessing module."
      ],
      "metadata": {
        "id": "z8KFVojbf5vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set ?\n",
        "    - a test set is a portion of the data that is withheld during model training and used solely to evaluate the model's performance on unseen data after it has been trained."
      ],
      "metadata": {
        "id": "R8pCFTQrf570"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "    - To split data for model fitting, training, and testing in Python, use the train_test_split function from the scikit-learn library, specifying the data, target, and desired test size (e.g., 20% or 25%).\n",
        "      # Assuming 'X' contains your features and 'y' contains your target variable\n",
        "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #test_size=0.2 means 20% for testing, random_state=42 for reproducibility.\n",
        "    \n",
        "      **train_test_split(X, y, ...):**\n",
        "This function takes your data (X and y), and parameters to control the split.\n",
        "     \n",
        "      **test_size:**\n",
        "      Specifies the proportion of the data to allocate to the test set (e.g., 0.2 for 20%). The remaining data is used for training.\n",
        "      \n",
        "      **random_state:**\n",
        "       An optional parameter that sets a seed for the random number generator, ensuring that the split is reproducible. Without it, the split will vary each time you run the code.\n",
        "        \n",
        "         **Approach a Machine Learning problem**\n",
        "         \n",
        "     The first step can be just to have a high-level understanding of the data. Try to find the answers to some simple questions.\n",
        "\n",
        "     What do these features mean?\n",
        "\n",
        "     How was the data collected and what was the purpose for collecting the data ( This will help you in understanding any sampling anomalies)?\n",
        "\n",
        "      Do we already know what we are trying to find out or do we need to lay down the problem ourselves? ( what kind of a problem we are trying to solve — supervised, unsupervised, prescriptive, or re-enforcement )"
      ],
      "metadata": {
        "id": "uROVp_tKf6GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  - Data Analysis (EDA) before model fitting is crucial because it helps identify data quality issues (missing values, outliers, inconsistencies), understand data structure and relationships, and informs data cleaning and feature selection, ultimately leading to more accurate and reliable models."
      ],
      "metadata": {
        "id": "NHfVsemjf6v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "    - **Correlation:-**  Correlation is a statistical term that describes the relationship or association between two or more variables.\n",
        "It indicates whether the variables tend to move together (positive correlation), move in opposite directions (negative correlation), or have no relationship (zero correlation)."
      ],
      "metadata": {
        "id": "VQL2k0r4f7DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "   - **negative correlation:-**Negative correlation, also known as inverse correlation, means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "For example, if the price of a product increases, the demand for that product might decrease, indicating a negative correlation between price and demand.\n"
      ],
      "metadata": {
        "id": "DU_gsCN2hZJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "   - To use the corrcoef() function, you need to pass in two arrays of data, one for each variable. The function will return a correlation matrix, which is a square matrix where the diagonal elements are always 1 and the off-diagonal elements indicate the correlations between different variables."
      ],
      "metadata": {
        "id": "H50z904mheYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "   - Causation means one event directly results in another, while correlation simply indicates a relationship between two events, where one doesn't necessarily cause the other. For example, while ice cream sales and shark attacks might be correlated (both increase during summer), ice cream sales do not cause shark attacks, but rather both are influenced by the third variable: sunny weather.\n",
        "   \n",
        "   Correlation and causation are one of the most important but confusing topics of statistics. Correlation gives the relationship between two variables, whereas causation means one event is cause due to another.\n",
        "    \n",
        "   **the difference between Correlation and Causation.** and Causation are two of the most important concepts to understand when it comes to understanding the world around us. But what’s the difference between the two?\n",
        "Simply put, Correlation is when two things happen together, while Causation is when one thing causes another thing to happen. So, for example, you might say that there is a correlation between ice cream sales and crime rates because you notice that they both seem to rise and fall together. This doesn’t mean that eating ice cream causes crime rates to go up (Causation), but it could suggest that another factor at play, like temperature, affects both of them.\n",
        "In this article, we’ll go over the key differences between Correlation and Causation and give you an example to get a better understanding of Correlation and Causation."
      ],
      "metadata": {
        "id": "9T9JQ-NTmN7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "   - An optimizer is an algorithm that adjusts a model's parameters (like weights and biases) during training to minimize the loss function and improve performance. Common optimizers include Stochastic Gradient Descent (SGD), Adam, and RMSprop.\n",
        "\n",
        "   **different types of optimizers:**\n",
        "\n",
        " **Gradient Descent (GD):**\n",
        "     \n",
        "      A fundamental optimization algorithm that iteratively moves towards the minimum of a function by calculating the gradient (slope) at the current point and moving in the opposite direction.\n",
        "     \n",
        "     **Example:** Imagine descending a mountain. GD would take a step in the direction opposite to the steepest slope to reach the valley bottom (minimum loss).\n",
        "\n",
        "   **Adaptive Optimizers:**\n",
        "       \n",
        "       These optimizers dynamically adjust the learning rate for each parameter based on its recent gradients, adapting to the landscape of the loss function.\n",
        "     \n",
        "     **Examples:**\n",
        "        1. Adagrad: Adapts the learning rate for each parameter based on the sum of the squares of its past gradients, giving smaller learning rates to frequently updated parameters and larger learning rates to less frequently updated parameters.\n",
        "        2. RMSprop: Similar to Adagrad, but it uses a moving average of the squared gradients to smooth out the learning rate updates, leading to more stable training.\n",
        "        3. Adam: Combines the ideas of both Adagrad and RMSprop, using both the first and second moments of the gradients to adapt the learning rate, often leading to faster and more stable convergence."
      ],
      "metadata": {
        "id": "GBh5dqj-mOX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model?\n",
        "   - linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features."
      ],
      "metadata": {
        "id": "mpHhr8-umOxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "   - The fit() method takes the training data as arguments, which can be one array in the case of unsupervised learning, or two arrays in the case of supervised learning.\n",
        "     \n",
        "       To train a model with fit() , you need to specify a loss function, an optimizer, and optionally, some metrics to monitor. The metrics argument should be a list -- your model can have any number of metrics.\n"
      ],
      "metadata": {
        "id": "I4Mm4wvymPAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "   - The model's function is to use the input data and predict the output. The target column must always contain numbers. However, before training the model, there are a few hyperparameters that need to be understood."
      ],
      "metadata": {
        "id": "r7E4hPkGmPhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "   - **Continuous Variables:**\n",
        "  \n",
        "      These variables represent quantities that can be measured and can take on any value within a certain range, including decimals and fractions.\n",
        "     \n",
        "     **Examples:** Height, weight, temperature, time, distance, salary, age.\n",
        "\n",
        "     **Categorical Variables:**\n",
        "\n",
        "     These variables represent qualities or characteristics that fall into distinct categories or groups.\n",
        "     \n",
        "     **Examples:**\n",
        "Gender (male/female), blood type (A, B, AB, O), color (red, blue, green), type of car (sedan, SUV, truck), yes/no questions."
      ],
      "metadata": {
        "id": "1vdV-6XvqICc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling is a data preprocessing technique that transforms the values of numerical features to a common scale, ensuring all features contribute equally to machine learning model training and improving model performance, especially for algorithms sensitive to feature ranges.\n",
        "\n",
        "     **Example:**\n",
        "Imagine you're building a model to predict house prices, and two features are \"number of bedrooms\" (ranging from 1 to 5) and \"house size in square feet\" (ranging from 500 to 5000). Without scaling, the model might be heavily influenced by the larger range of house sizes, potentially overshadowing the importance of the number of bedrooms. Feature scaling ensures that both features contribute equally to the prediction process."
      ],
      "metadata": {
        "id": "a_z0Z287r874"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "    -  The two most important scaling techniques are Normalization and Standardization.\n",
        "\n",
        "      When data is scaled using normalization, the transformed data can be calculated using this equation\n",
        "\n",
        "Equation\n",
        "\n",
        "         x(new)=x-x(min)/x(max)-x(min)\n",
        "\n",
        "   **Data Scaling Using Standardization**\n",
        "\n",
        "\n",
        "Ideally, standardization should be used when the data is distributed according to the normal or Guassian distribution. The standardized data can be calculated as follows:\n",
        "\n",
        "Equation\n",
        "          \n",
        "           x(std)= x-x(bar)/sigma x\n"
      ],
      "metadata": {
        "id": "PWVi297Er9YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5noOMslDxYzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "   - scikit-learn (sklearn), \"preprocessing\" refers to the process of transforming raw data into a format that is more suitable for machine learning algorithms, using utility functions and transformer classes within the sklearn.preprocessing module."
      ],
      "metadata": {
        "id": "_0G5T42xr9py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    - To split data for model fitting, training, and testing in Python, use the train_test_split function from the scikit-learn library, specifying the data, target, and desired test size (e.g., 20% or 25%)."
      ],
      "metadata": {
        "id": "uR9KVnWZr963"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "  - Data encoding is the process of converting information or data into a specific format for efficient storage, transmission, or processing, often for use in machine learning or digital communication."
      ],
      "metadata": {
        "id": "ku8zIP-er-vD"
      }
    }
  ]
}